{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELMoPretrainDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset_path: str, seq_len: int):\n",
    "        text = open(dataset_path, \"r\").readlines()\n",
    "        self.seq_len = seq_len\n",
    "        self.text, self.char2idx, self.idx2char, self.word2idx, self.idx2word = self.preprocess(text)\n",
    "        self.tokenized_chars, self.tokenized_words = self.tokenize(self.text)\n",
    "\n",
    "    def tokenize(self, text: list[str]) -> list[list[int]]:\n",
    "        char_tokenize = [[self.char2idx[char] for char in word] for word in text]\n",
    "        word_tokenize = [self.word2idx[word] for word in text]\n",
    "        return char_tokenize, word_tokenize\n",
    "    \n",
    "    def pad(self, sequence: list[list[int]]):\n",
    "        max_word_len = len(max(sequence, key=lambda x: len(x)))\n",
    "        for i in range(len(sequence)):\n",
    "            pad_len = max_word_len - len(sequence[i])\n",
    "            front_pad = pad_len // 2\n",
    "            back_pad = pad_len - front_pad\n",
    "            sequence[i] = ([0] * front_pad) + sequence[i] + ([0] * back_pad)\n",
    "\n",
    "    def preprocess(self, text: list[str]) -> list[str]:\n",
    "        to_remove = []\n",
    "        for i, line in enumerate(text):\n",
    "            if line == \" \\n\":\n",
    "                to_remove.append(i)\n",
    "            elif \"=\" in line:\n",
    "                to_remove.append(i)\n",
    "\n",
    "        for idx in to_remove[::-1]:\n",
    "            del text[idx]\n",
    "        \n",
    "        text = \" \".join(text).lower()\n",
    "        text = text.translate(str.maketrans('', '', '!\"#$%&\\'()*+-./:=?@[\\\\]^_`{|}~'))\n",
    "        text = \"\".join([i for i in text if (not i.isdigit()) and i.isascii()])\n",
    "\n",
    "        char2idx = {char: i + 1 for i, char in enumerate(sorted(list(set(list(text)))))}\n",
    "        char2idx[\"<pad>\"] = 0\n",
    "        idx2char = {value: key for key, value in char2idx.items()}\n",
    "        text = text.split()\n",
    "\n",
    "        word2idx = {word: i for i, word in enumerate(sorted(list(set(text))))}\n",
    "        idx2word = {value: key for key, value in word2idx.items()}\n",
    "        return text, char2idx, idx2char, word2idx, idx2word\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.text) - self.seq_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src, tgt = self.tokenized_chars[idx: idx + self.seq_len], self.tokenized_words[idx+1: idx + self.seq_len + 1]\n",
    "        self.pad(src)\n",
    "        return torch.tensor(src), torch.tensor(tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PadCollate:\n",
    "\n",
    "    def __init__(self, dim=0):\n",
    "        self.dim = dim\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        max_len = max(batch, key=lambda x: x[0].shape[-1])[0].shape[-1]\n",
    "        seq_len = batch[0][1].shape[0]\n",
    "        srcs, tgts = [], []\n",
    "        for src, tgt in batch:\n",
    "            pad_len = max_len - src.shape[-1]\n",
    "            front_pad = pad_len // 2\n",
    "            back_pad = pad_len - front_pad\n",
    "            srcs.append(torch.cat([torch.zeros(seq_len, front_pad), src, torch.zeros(seq_len, back_pad)], dim=-1))\n",
    "            tgts.append(tgt)\n",
    "        return torch.stack(srcs).type(torch.long), torch.stack(tgts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterConvolutions(nn.Module):\n",
    "\n",
    "    def __init__(self, in_size, embed_dim):\n",
    "        super(CharacterConvolutions, self).__init__()\n",
    "        self.embedding = nn.Embedding(in_size, embed_dim)\n",
    "        conv_layer_params = [[1, 32], [2, 32], [3, 64], [4, 128], [5, 256], [6, 512], [7, 1024]]\n",
    "        self.conv_layers = nn.ModuleList([nn.Conv2d(embed_dim, out_dim, (1, ksize)) for ksize, out_dim in conv_layer_params])\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    # x has shape [batch_size, seq_len, characters]\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) # [batch_size, seq_len, characters, char_embed_dim]\n",
    "        x = x.permute(0, 3, 1, 2) # [batch_size, char_embed_dim, seq_len, characters]\n",
    "        convs = [c(x) for c in self.conv_layers]\n",
    "        pools = [self.relu(F.max_pool2d(c, kernel_size=(1, c.shape[-1]))).squeeze(-1).permute(0, 2, 1) for c in convs]\n",
    "        return torch.cat(pools, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELMoLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, dropout):\n",
    "        super(ELMoLSTM, self).__init__()\n",
    "        self.in_proj = nn.Linear(input_size, hidden_size)\n",
    "        self.lstm1 = nn.LSTM(hidden_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.middle_proj = nn.Linear(hidden_size * 2, input_size)\n",
    "        self.lstm2 = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.out_proj = nn.Linear(hidden_size * 2, input_size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.in_proj(x)\n",
    "        layer1 = self.middle_proj(self.lstm1(x)[0] + x.repeat(1, 1, 2))\n",
    "        layer2 = self.out_proj(self.lstm2(self.dropout(layer1))[0])\n",
    "        return layer1, layer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELMo(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, char_embed_dim=16, output_size=128, hidden_size=1024, dropout=0.5):\n",
    "        super(ELMo, self).__init__()\n",
    "        self.character_convolutions = CharacterConvolutions(input_size, char_embed_dim)\n",
    "        self.highway = nn.Linear(2048, 2048)\n",
    "        self.highway_gate = nn.Linear(2048, 2048)\n",
    "        self.in_proj = nn.Linear(2048, output_size)\n",
    "        self.elmo_lstm = ELMoLSTM(output_size, hidden_size=hidden_size, dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.character_convolutions(x)\n",
    "        h = self.highway(x)\n",
    "        h_gate = F.sigmoid(self.highway_gate(x))\n",
    "        layer1 = self.in_proj((h * h_gate) + (x * (1 - h_gate)))\n",
    "        layer2, layer3 = self.elmo_lstm(layer1)\n",
    "        return layer1, layer2, layer3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELMoPretrainModel(nn.Module):\n",
    "\n",
    "    def __init__(self, num_chars, output_size, num_words, dropout=0.5):\n",
    "        super(ELMoPretrainModel, self).__init__()\n",
    "        self.elmo = ELMo(num_chars, output_size=output_size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.out_proj = nn.Linear(output_size, num_words)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, _, x = self.elmo(x)\n",
    "        return self.out_proj(self.dropout(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ELMoPretrainDataset(\"wikitext-2/wiki.train.tokens\", seq_len=100)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=PadCollate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ELMoPretrainModel(len(dataset.char2idx), 128, len(dataset.word2idx)).to(\"mps\")\n",
    "crit = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(model.parameters(), 3e-6)\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : [0/100]:   0%|          | 1/55723 [00:02<32:24:43,  2.09s/it, loss=10.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.233040809631348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : [0/100]:   0%|          | 2/55723 [00:03<25:59:49,  1.68s/it, loss=10.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.225953102111816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : [0/100]:   0%|          | 3/55723 [00:04<23:17:06,  1.50s/it, loss=10.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.229717254638672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : [0/100]:   0%|          | 4/55723 [00:06<22:08:29,  1.43s/it, loss=10.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.229687690734863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : [0/100]:   0%|          | 5/55723 [00:07<21:36:31,  1.40s/it, loss=10.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.226344108581543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : [0/100]:   0%|          | 6/55723 [00:08<21:14:49,  1.37s/it, loss=10.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.229728698730469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : [0/100]:   0%|          | 6/55723 [00:09<25:34:45,  1.65s/it, loss=10.2]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m2/jysgcgj57vn69541g8m0qz_h0000gn/T/ipykernel_45753/2325481352.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
      "\u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in range(EPOCHS):\n",
    "    loop = tqdm(loader, total=len(loader), position=0)\n",
    "    loop.set_description(f\"Epoch : [{e}/{EPOCHS}]\")\n",
    "    for src, tgt in loop:\n",
    "        src, tgt = src.to(\"mps\"), tgt.to(\"mps\")\n",
    "        opt.zero_grad()\n",
    "        yhat = model(src)\n",
    "        loss = crit(yhat.view(-1, yhat.shape[-1]), tgt.view(-1))\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        opt.step()\n",
    "        print(loss.item())\n",
    "        loop.set_postfix(loss = loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
